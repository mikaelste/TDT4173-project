{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Kaggle: Forrest Gump </h3>\n",
    "\n",
    "<p>Mikael Steenbuch, studentid: 564304\n",
    "\n",
    "Henning Drøpping, studentid:564571\n",
    "\n",
    "Mats Alexander Nissen-Lie, studentid:564393\n",
    "<p>\n",
    "\n",
    "CSV name: gluon_3_pivot_drop_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "print(\"Current working directory:\", current_dir)\n",
    "\n",
    "\n",
    "PATH = \"/Users/matsalexander/Desktop/Forest Gump/\"\n",
    "# Estimate\n",
    "X_train_estimated_a: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + 'A/X_train_estimated.parquet')\n",
    "X_train_estimated_b: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"B/X_train_estimated.parquet\")\n",
    "X_train_estimated_c: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"C/X_train_estimated.parquet\")\n",
    "\n",
    "# Test estimates\n",
    "X_test_estimated_a: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"A/X_test_estimated.parquet\")\n",
    "X_test_estimated_b: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"B/X_test_estimated.parquet\")\n",
    "X_test_estimated_c: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"C/X_test_estimated.parquet\")\n",
    "\n",
    "# Observations\n",
    "X_train_observed_a: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"A/X_train_observed.parquet\")\n",
    "X_train_observed_b: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"B/X_train_observed.parquet\")\n",
    "X_train_observed_c: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"C/X_train_observed.parquet\")\n",
    "\n",
    "# Targets\n",
    "Y_train_observed_a: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"A/train_targets.parquet\")\n",
    "Y_train_observed_b: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"B/train_targets.parquet\")\n",
    "Y_train_observed_c: pd.DataFrame = pd.read_parquet(\n",
    "    PATH + \"C/train_targets.parquet\")\n",
    "\n",
    "test_df_example = pd.read_csv(PATH + \"test.csv\")\n",
    "\n",
    "best_submission: pd.DataFrame = pd.read_csv(\n",
    "    PATH + \"mikael/submissions/fourth_submission.csv\")\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_combined_data(self, test_data=False):\n",
    "        locations = [\"A\", \"B\", \"C\"]\n",
    "        dfs = []\n",
    "        for index, location in enumerate(locations):\n",
    "            if test_data:\n",
    "                dfs.append(self.get_test_data(location))\n",
    "            else:\n",
    "                dfs.append(self.get_data(location))\n",
    "\n",
    "            dfs[index] = self.onehot_location(dfs[index], location)\n",
    "        df = pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "        if test_data:\n",
    "            return df\n",
    "        return df[[c for c in df if c not in ['pv_measurement']] +  # pv measurement is the target and is at the end columns\n",
    "                  ['pv_measurement']]\n",
    "\n",
    "    def get_data(self, location: str, keeptime=False) -> pd.DataFrame:\n",
    "        train, targets = self.get_training_data_by_location(location)\n",
    "        return self.handle_data(train, targets, keeptime=keeptime)\n",
    "\n",
    "    def get_test_data(self, location: str) -> pd.DataFrame:\n",
    "        test_data = self.get_test_data_by_location(location)\n",
    "        return self.handle_data(test_data)\n",
    "\n",
    "    def handle_data(self, df, targets=pd.DataFrame(), keeptime=False):\n",
    "        df[\"date_calc\"] = pd.to_datetime(df[\"date_calc\"])\n",
    "        df[\"date_forecast\"] = pd.to_datetime(df[\"date_forecast\"])\n",
    "\n",
    "        df = self.drop_columns(df)\n",
    "        df = self.grouped_by_hour(df)\n",
    "\n",
    "        df = self.unzip_date_feature(df)\n",
    "        df = self.onehot_estimated(df)\n",
    "\n",
    "        df[\"time\"] = df[\"date_forecast\"]\n",
    "        df.drop([\"date_forecast\"], axis=1, inplace=True)\n",
    "        if not targets.empty:\n",
    "            df = self.merge_train_target(df, targets)\n",
    "\n",
    "        df.drop(columns=[\"time\"], axis=1, inplace=True)\n",
    "\n",
    "        df = self.absolute_values(df)\n",
    "        return df\n",
    "\n",
    "    # –––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– helper funciton ––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
    "\n",
    "    def get_training_data_by_location(self, location):\n",
    "        if location == \"A\":\n",
    "            X_train_observed_x = X_train_observed_a\n",
    "            X_train_estimated_x = X_train_estimated_a\n",
    "            Y_train_x = Y_train_observed_a\n",
    "        elif location == \"B\":\n",
    "            X_train_observed_x = X_train_observed_b\n",
    "            X_train_estimated_x = X_train_estimated_b\n",
    "            Y_train_x = Y_train_observed_b\n",
    "        elif location == \"C\":\n",
    "            X_train_observed_x = X_train_observed_c\n",
    "            X_train_estimated_x = X_train_estimated_c\n",
    "            Y_train_x = Y_train_observed_c\n",
    "        else:\n",
    "            raise Exception(\"location must be A, B or C\")\n",
    "        train = pd.concat(\n",
    "            [X_train_observed_x, X_train_estimated_x]).reset_index(drop=True)\n",
    "        return train, Y_train_x\n",
    "\n",
    "    def get_test_data_by_location(self, location: str) -> pd.DataFrame:\n",
    "        if location == \"A\":\n",
    "            df = X_test_estimated_a\n",
    "        elif location == \"B\":\n",
    "            df = X_test_estimated_b\n",
    "        elif location == \"C\":\n",
    "            df = X_test_estimated_c\n",
    "        else:\n",
    "            raise Exception(\"location must be A, B or C\")\n",
    "        return df.copy()\n",
    "\n",
    "    def unzip_date_feature(self, df: pd.DataFrame, date_column: str = \"date_forecast\"):\n",
    "        df[date_column] = pd.to_datetime(df[date_column])\n",
    "        df[\"day_of_year\"] = df[\"date_forecast\"].dt.day_of_year\n",
    "        df[\"hour\"] = df[\"date_forecast\"].dt.hour\n",
    "        # df[\"month\"] = df[\"date_forecast\"].dt.month\n",
    "        return df\n",
    "\n",
    "    def add_time_since_calucation(self, df):  # denne er ikke så dum.\n",
    "        df[\"date_calc\"] = pd.to_datetime(df[\"date_calc\"])\n",
    "        df[\"calculated_ago\"] = (\n",
    "            df[\"date_forecast\"] - df[\"date_calc\"]).dt.total_seconds()\n",
    "        df[\"calculated_ago\"] = df[\"calculated_ago\"].fillna(\n",
    "            0) / 60/30\n",
    "        return df\n",
    "\n",
    "    def onehot_estimated(self, df):\n",
    "        df[\"estimated\"] = 0  # Initialize both columns to 0\n",
    "        df[\"observed\"] = 0\n",
    "        estimated_mask = df[\"date_calc\"].notna()\n",
    "        df.loc[estimated_mask, \"estimated\"] = 1\n",
    "        df.loc[~estimated_mask, \"observed\"] = 1\n",
    "        df.drop(columns=[\"date_calc\"], inplace=True)\n",
    "        return df\n",
    "\n",
    "    def onehot_location(self, df, location):\n",
    "        if location == \"A\":\n",
    "            df[\"A\"], df[\"B\"], df[\"C\"] = 1, 0, 0\n",
    "        elif location == \"B\":\n",
    "            df[\"A\"], df[\"B\"], df[\"C\"] = 0, 1, 0\n",
    "        elif location == \"C\":\n",
    "            df[\"A\"], df[\"B\"], df[\"C\"] = 0, 0, 1\n",
    "        return df\n",
    "\n",
    "    def grouped_by_hour(self, df: pd.DataFrame, date_column: str = \"date_forecast\") -> pd.DataFrame:\n",
    "        # Group by hour and aggregate the values into lists for all columns\n",
    "        df['date_hour'] = df[date_column].dt.to_period('H')\n",
    "        df[\"min\"] = df[date_column].dt.minute\n",
    "        df.drop(columns=[date_column], inplace=True)\n",
    "\n",
    "        # Use pivot_table to combine rows with the same date and hour\n",
    "        pivot_df = df.pivot_table(index=['date_hour'], columns=[\n",
    "                                  'min'], values=df.columns, aggfunc='first')\n",
    "\n",
    "        pivot_df.index.names = [date_column]\n",
    "\n",
    "        # Reset index to make 'date' a regular column\n",
    "        pivot_df.columns = pivot_df.columns.to_flat_index()\n",
    "        pivot_df.reset_index(inplace=True)\n",
    "\n",
    "        pivot_df[\"date_forecast\"] = pivot_df[\"date_forecast\"].dt.to_timestamp()\n",
    "\n",
    "        pivot_df[\"date_calc\"] = pivot_df[('date_calc', 0)]\n",
    "        pivot_df.drop(columns=[\n",
    "            ('date_calc', c) for c in range(0, 60, 15)\n",
    "        ], inplace=True)\n",
    "\n",
    "        pivot_df.columns = [str(col) for col in pivot_df.columns]\n",
    "\n",
    "        return pivot_df\n",
    "\n",
    "    def grouped_by_hour_old(self, df: pd.DataFrame, date_column: str = \"date_forecast\"):\n",
    "        df = df.groupby(pd.Grouper(key=date_column, freq=\"1H\")\n",
    "                        ).mean(numeric_only=True)\n",
    "        all_nan_mask = df.isnull().all(axis=1)\n",
    "        df = df[~all_nan_mask]\n",
    "        return df.reset_index()\n",
    "\n",
    "    def merge_train_target(self, x, y):\n",
    "        # henning får med alle pv measurments selv om han merger på inner time. Fordi resample fyller nan rows for alle timer som ikke er i datasettet.\n",
    "        merged = pd.merge(x, y, on=\"time\", how=\"right\")\n",
    "        mask = merged[\"pv_measurement\"].notna()\n",
    "        merged = merged.loc[mask].reset_index(drop=True)\n",
    "        return merged\n",
    "\n",
    "    def absolute_values(self, df: pd.DataFrame):\n",
    "        columns = list(df.columns)\n",
    "        df[columns] = df[columns].abs()\n",
    "        df = df.replace(-0.0, 0.0)\n",
    "        return df\n",
    "\n",
    "    def remove_consecutive_measurments_new(self, df: pd.DataFrame, consecutive_threshold=3, consecutive_threshold_zero=12,  return_removed=False):\n",
    "        if consecutive_threshold < 2:\n",
    "            return df\n",
    "\n",
    "        column_to_check = 'pv_measurement'\n",
    "\n",
    "        mask = (df[column_to_check] != df[column_to_check].shift(1)).cumsum()\n",
    "        df['consecutive_group'] = df.groupby(\n",
    "            mask).transform('count')[column_to_check]\n",
    "\n",
    "        df[\"is_first_in_consecutive_group\"] = False\n",
    "        df['is_first_in_consecutive_group'] = df['consecutive_group'] != df['consecutive_group'].shift(\n",
    "            1)\n",
    "\n",
    "        # masks to remove rows\n",
    "        mask_non_zero = (df['consecutive_group'] >= consecutive_threshold) & (\n",
    "            df[\"pv_measurement\"] > 0) & (df[\"is_first_in_consecutive_group\"] == False)  # or df[\"direct_rad:W\"] == 0)\n",
    "\n",
    "        mask_zero = (df['consecutive_group'] >= consecutive_threshold_zero) & (\n",
    "            df[\"pv_measurement\"] == 0) & (df[\"is_first_in_consecutive_group\"] == False)\n",
    "\n",
    "        mask = mask_non_zero | mask_zero\n",
    "\n",
    "        if return_removed:\n",
    "            return df[mask]\n",
    "\n",
    "        df = df.loc[~mask]\n",
    "\n",
    "        df = df.drop(columns=[\"consecutive_group\",\n",
    "                     \"is_first_in_consecutive_group\"])\n",
    "\n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "    def remove_consecutive_measurments_new_new(self, df: pd.DataFrame, consecutive_threshold=3, consecutive_threshold_zero=12, consecutive_threshold_zero_no_rad=20, return_removed=False):\n",
    "        if consecutive_threshold < 2:\n",
    "            return df\n",
    "\n",
    "        column_to_check = 'pv_measurement'\n",
    "\n",
    "        mask = (df[column_to_check] != df[column_to_check].shift(1)).cumsum()\n",
    "        df['consecutive_group'] = df.groupby(\n",
    "            mask).transform('count')[column_to_check]\n",
    "\n",
    "        df[\"is_first_in_consecutive_group\"] = False\n",
    "        df['is_first_in_consecutive_group'] = df['consecutive_group'] != df['consecutive_group'].shift(\n",
    "            1)\n",
    "        rad_cols = [f\"('direct_rad:W', {c})\" for c in range(0, 60, 15)]\n",
    "        df[\"direct_rad:W\"] = df[rad_cols].mean(axis=1)\n",
    "        # masks to remove rows\n",
    "        mask_non_zero = (df['consecutive_group'] >= consecutive_threshold) & (\n",
    "            df[\"pv_measurement\"] > 0) & (df[\"is_first_in_consecutive_group\"] == False)  # or df[\"direct_rad:W\"] == 0)\n",
    "\n",
    "        tol = 10\n",
    "        mask_zero = (df['consecutive_group'] >= consecutive_threshold_zero) & (\n",
    "            df[\"pv_measurement\"] == 0) & (df[\"direct_rad:W\"] > tol)\n",
    "\n",
    "        mask_zero_no_rad = (df['consecutive_group'] >= consecutive_threshold_zero_no_rad) & (\n",
    "            df[\"pv_measurement\"] == 0) & (df[\"direct_rad:W\"] < tol)\n",
    "        mask = mask_non_zero | mask_zero | mask_zero_no_rad\n",
    "\n",
    "        if return_removed:\n",
    "            return df[mask]\n",
    "\n",
    "        df = df.loc[~mask]\n",
    "\n",
    "        df = df.drop(columns=[\"consecutive_group\",\n",
    "                     \"is_first_in_consecutive_group\", \"direct_rad:W\"])\n",
    "\n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "    def compare_mae(self, df: pd.DataFrame):\n",
    "        best_submission: pd.DataFrame = pd.read_csv(\n",
    "            PATH+\"mats/submissions/gluon_3_remove_consecutive_measurements_66.csv\")\n",
    "        best_submission = best_submission[[\"prediction\"]]\n",
    "\n",
    "        if best_submission.shape != df.shape:\n",
    "            print(\"best_submission\", best_submission.shape)\n",
    "            print(\"df\", df.shape)\n",
    "            raise Exception(\"Dataframe shape must be the same\")\n",
    "\n",
    "        return mean_absolute_error(\n",
    "            best_submission[\"prediction\"], df[\"prediction\"])\n",
    "\n",
    "    def drop_columns(self, df: pd.DataFrame):\n",
    "        drop = [\n",
    "            # wind speed vector u, available up to 20000 m, from 1000 hPa to 10 hPa and on flight levels FL10-FL900[m/s] does not make sens at surfece level\n",
    "            \"wind_speed_w_1000hPa:ms\",\n",
    "            \"wind_speed_u_10m:ms\",  # same as above\n",
    "            \"wind_speed_v_10m:ms\",  # same as above\n",
    "            \"snow_density:kgm3\",\n",
    "            \"snow_drift:idx\",  # denne er ny. Fikk 140.9 uten.\n",
    "            \"cloud_base_agl:m\"\n",
    "        ]\n",
    "        shared_columns = list(set(df.columns) & set(drop))\n",
    "        df = df.drop(columns=shared_columns)\n",
    "        return df\n",
    "\n",
    "    def post_processing(self, df: pd.DataFrame, prediction_column: str = \"prediction_label\"):\n",
    "        df = df[[prediction_column]].rename(\n",
    "            columns={prediction_column: \"prediction\"}).reset_index(drop=True).rename_axis(index=\"id\")\n",
    "\n",
    "        df[\"prediction\"] = df[\"prediction\"].clip(lower=0)\n",
    "        return df\n",
    "\n",
    "pipin = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_0 = pipin.get_data(\"A\")\n",
    "df2_0 = pipin.get_data(\"B\")\n",
    "df3_0 = pipin.get_data(\"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold1 = 4\n",
    "threshold2 = 12\n",
    "threshold3 = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_consecutive_measurments\n",
    "# 6/6 er henning sin beste\n",
    "df1_0 = pipin.remove_consecutive_measurments_new_new(df1_0, threshold1, threshold2, threshold3 ) #tipper oslo\n",
    "df2_0 = pipin.remove_consecutive_measurments_new_new(df2_0, threshold1, threshold2, threshold3 ) #tipper Trondheim\n",
    "df3_0 = pipin.remove_consecutive_measurments_new_new(df3_0, threshold1, threshold2, threshold3 ) #tipper Trondheim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33850, 161) (24155, 161) (19875, 161)\n"
     ]
    }
   ],
   "source": [
    "# df1_0 = pipin.drop_columns(df1_0) #gjort i pipeline\n",
    "# df2_0 = pipin.drop_columns(df2_0)\n",
    "# df3_0 = pipin.drop_columns(df3_0)\n",
    "print(df1_0.shape, df2_0.shape, df3_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pv_measurement'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_0.head()\n",
    "test1 = pipin.get_test_data(\"A\")\n",
    "def compare_columns(df1, df2):\n",
    "    # diff = set(df1.columns).difference(set(df2.columns))\n",
    "    diff = set(df1.columns)-(set(df2.columns))\n",
    "    if len(diff) > 1:\n",
    "        print(\"df1 har flere kolonner enn df2\")\n",
    "        raise ValueError()\n",
    "    return diff\n",
    "compare_columns(df1_0, test1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = TabularDataset(df1_0)\n",
    "train2 = TabularDataset(df2_0)\n",
    "train3 = TabularDataset(df3_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20231111_092703/\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20231111_092703/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 22.5.0: Thu Jun  8 22:22:20 PDT 2023; root:xnu-8796.121.3~7/RELEASE_ARM64_T6000\n",
      "Disk Space Avail:   795.16 GB / 994.66 GB (79.9%)\n",
      "Train Data Rows:    33850\n",
      "Train Data Columns: 160\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5733.42, 0.0, 634.97254, 1168.75397)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    18993.27 MB\n",
      "\tTrain Data (Original)  Memory Usage: 22.21 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 4): [\"('elevation:m', 15)\", \"('elevation:m', 30)\", \"('elevation:m', 45)\", \"('snow_melt_10min:mm', 15)\"]\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 4 | [\"('elevation:m', 15)\", \"('elevation:m', 30)\", \"('elevation:m', 45)\", \"('snow_melt_10min:mm', 15)\"]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 156 | [\"('absolute_humidity_2m:gm3', 0)\", \"('absolute_humidity_2m:gm3', 15)\", \"('absolute_humidity_2m:gm3', 30)\", \"('absolute_humidity_2m:gm3', 45)\", \"('air_density_2m:kgm3', 0)\", ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 155 | [\"('absolute_humidity_2m:gm3', 0)\", \"('absolute_humidity_2m:gm3', 15)\", \"('absolute_humidity_2m:gm3', 30)\", \"('absolute_humidity_2m:gm3', 45)\", \"('air_density_2m:kgm3', 0)\", ...]\n",
      "\t\t('int', ['bool']) :   1 | [\"('elevation:m', 0)\"]\n",
      "\t0.5s = Fit runtime\n",
      "\t156 features in original data used to generate 156 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.56 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
      "\t-221.6095\t = Validation score   (-mean_absolute_error)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t1.94s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ...\n",
      "\t-221.2823\t = Validation score   (-mean_absolute_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t1.91s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-160.3782\t = Validation score   (-mean_absolute_error)\n",
      "\t95.36s\t = Training   runtime\n",
      "\t38.58s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-168.2892\t = Validation score   (-mean_absolute_error)\n",
      "\t73.97s\t = Training   runtime\n",
      "\t44.09s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ...\n",
      "\t-181.063\t = Validation score   (-mean_absolute_error)\n",
      "\t57.4s\t = Training   runtime\n",
      "\t1.58s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-172.7714\t = Validation score   (-mean_absolute_error)\n",
      "\t420.98s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ...\n",
      "\t-180.6483\t = Validation score   (-mean_absolute_error)\n",
      "\t12.58s\t = Training   runtime\n",
      "\t1.69s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-185.3444\t = Validation score   (-mean_absolute_error)\n",
      "\t18.83s\t = Training   runtime\n",
      "\t0.55s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-175.9416\t = Validation score   (-mean_absolute_error)\n",
      "\t264.02s\t = Training   runtime\n",
      "\t6.35s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-173.8304\t = Validation score   (-mean_absolute_error)\n",
      "\t43.7s\t = Training   runtime\n",
      "\t0.39s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-164.4634\t = Validation score   (-mean_absolute_error)\n",
      "\t225.57s\t = Training   runtime\n",
      "\t200.11s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-157.6345\t = Validation score   (-mean_absolute_error)\n",
      "\t0.33s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 9 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-161.6671\t = Validation score   (-mean_absolute_error)\n",
      "\t4.08s\t = Training   runtime\n",
      "\t0.39s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-159.7003\t = Validation score   (-mean_absolute_error)\n",
      "\t2.93s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ...\n",
      "\t-159.0507\t = Validation score   (-mean_absolute_error)\n",
      "\t66.76s\t = Training   runtime\n",
      "\t1.69s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-160.0795\t = Validation score   (-mean_absolute_error)\n",
      "\t15.15s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ...\n",
      "\t-158.1497\t = Validation score   (-mean_absolute_error)\n",
      "\t14.64s\t = Training   runtime\n",
      "\t1.77s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-159.2539\t = Validation score   (-mean_absolute_error)\n",
      "\t19.27s\t = Training   runtime\n",
      "\t0.52s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-158.1786\t = Validation score   (-mean_absolute_error)\n",
      "\t5.88s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-159.2214\t = Validation score   (-mean_absolute_error)\n",
      "\t25.04s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-160.4664\t = Validation score   (-mean_absolute_error)\n",
      "\t14.98s\t = Training   runtime\n",
      "\t0.66s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ...\n",
      "\t-155.4437\t = Validation score   (-mean_absolute_error)\n",
      "\t0.21s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1461.4s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231111_092703/\")\n"
     ]
    }
   ],
   "source": [
    "predictor1 = TabularPredictor(label=\"pv_measurement\", eval_metric='mean_absolute_error').fit(\n",
    "    train1,\n",
    "    # time_limit=60*60*1.5,\n",
    "    # hyperparameters='extrme', \n",
    "    presets='best_quality',\n",
    "    # tuning_data = tuning1,\n",
    "    # use_bag_holdout=True,\n",
    "    # num_bag_folds= 6,\n",
    "    # refit_full = True,\n",
    "    # auto_stack = True,\n",
    "    # num_bag_sets= 10,\n",
    "    # set_best_to_refit_full= True,\n",
    "    # num_stack_levels = 2,\n",
    "    # verbosity = 3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = pipin.get_test_data(\"A\")\n",
    "test_data1 = TabularDataset(test1)\n",
    "\n",
    "\n",
    "pred1 = pd.DataFrame(predictor1.predict(test_data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20231111_095133/\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20231111_095133/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 22.5.0: Thu Jun  8 22:22:20 PDT 2023; root:xnu-8796.121.3~7/RELEASE_ARM64_T6000\n",
      "Disk Space Avail:   791.46 GB / 994.66 GB (79.6%)\n",
      "Train Data Rows:    24155\n",
      "Train Data Columns: 160\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (1152.3, 0.0, 114.94519, 218.12955)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    19396.27 MB\n",
      "\tTrain Data (Original)  Memory Usage: 15.85 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 3): [\"('elevation:m', 15)\", \"('elevation:m', 30)\", \"('elevation:m', 45)\"]\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 3 | [\"('elevation:m', 15)\", \"('elevation:m', 30)\", \"('elevation:m', 45)\"]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 157 | [\"('absolute_humidity_2m:gm3', 0)\", \"('absolute_humidity_2m:gm3', 15)\", \"('absolute_humidity_2m:gm3', 30)\", \"('absolute_humidity_2m:gm3', 45)\", \"('air_density_2m:kgm3', 0)\", ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 156 | [\"('absolute_humidity_2m:gm3', 0)\", \"('absolute_humidity_2m:gm3', 15)\", \"('absolute_humidity_2m:gm3', 30)\", \"('absolute_humidity_2m:gm3', 45)\", \"('air_density_2m:kgm3', 0)\", ...]\n",
      "\t\t('int', ['bool']) :   1 | [\"('elevation:m', 0)\"]\n",
      "\t0.4s = Fit runtime\n",
      "\t157 features in original data used to generate 157 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 15.48 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.46s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
      "\t-35.1617\t = Validation score   (-mean_absolute_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.66s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ...\n",
      "\t-35.0719\t = Validation score   (-mean_absolute_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.64s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-24.5655\t = Validation score   (-mean_absolute_error)\n",
      "\t41.45s\t = Training   runtime\n",
      "\t40.62s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-25.9901\t = Validation score   (-mean_absolute_error)\n",
      "\t64.74s\t = Training   runtime\n",
      "\t7.65s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ...\n",
      "\t-28.3838\t = Validation score   (-mean_absolute_error)\n",
      "\t42.43s\t = Training   runtime\n",
      "\t1.0s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-26.6262\t = Validation score   (-mean_absolute_error)\n",
      "\t362.87s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ...\n",
      "\t-28.2172\t = Validation score   (-mean_absolute_error)\n",
      "\t8.33s\t = Training   runtime\n",
      "\t1.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-29.166\t = Validation score   (-mean_absolute_error)\n",
      "\t13.67s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-27.3581\t = Validation score   (-mean_absolute_error)\n",
      "\t213.95s\t = Training   runtime\n",
      "\t7.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-27.9206\t = Validation score   (-mean_absolute_error)\n",
      "\t29.33s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-25.47\t = Validation score   (-mean_absolute_error)\n",
      "\t209.65s\t = Training   runtime\n",
      "\t30.58s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-24.2549\t = Validation score   (-mean_absolute_error)\n",
      "\t0.19s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 9 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-25.2176\t = Validation score   (-mean_absolute_error)\n",
      "\t4.46s\t = Training   runtime\n",
      "\t0.39s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-24.8233\t = Validation score   (-mean_absolute_error)\n",
      "\t3.45s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ...\n",
      "\t-24.4003\t = Validation score   (-mean_absolute_error)\n",
      "\t49.94s\t = Training   runtime\n",
      "\t1.14s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-24.9794\t = Validation score   (-mean_absolute_error)\n",
      "\t20.87s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ...\n",
      "\t-24.3182\t = Validation score   (-mean_absolute_error)\n",
      "\t9.58s\t = Training   runtime\n",
      "\t1.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-24.485\t = Validation score   (-mean_absolute_error)\n",
      "\t13.85s\t = Training   runtime\n",
      "\t0.41s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-24.6235\t = Validation score   (-mean_absolute_error)\n",
      "\t6.76s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-24.9809\t = Validation score   (-mean_absolute_error)\n",
      "\t21.16s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-24.8033\t = Validation score   (-mean_absolute_error)\n",
      "\t12.71s\t = Training   runtime\n",
      "\t0.5s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ...\n",
      "\t-23.9648\t = Validation score   (-mean_absolute_error)\n",
      "\t0.22s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1185.05s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231111_095133/\")\n"
     ]
    }
   ],
   "source": [
    "predictor2 = TabularPredictor(label=\"pv_measurement\", eval_metric='mean_absolute_error').fit(\n",
    "    train2,\n",
    "\n",
    "    presets='best_quality', \n",
    "    # hyperparameters='very_large', \n",
    "    # time_limit=60*60*1.5\n",
    "    # tuning_data = tuning2,\n",
    "    # use_bag_holdout=True,\n",
    "    # num_bag_folds= 6,\n",
    "    # refit_full = True,\n",
    "    # auto_stack = True,\n",
    "    # num_bag_sets= 10,\n",
    "    # set_best_to_refit_full= True,\n",
    "    # num_stack_levels = ,2\n",
    "    # verbosity = 3\n",
    "    )\n",
    "\n",
    "# tuning_data\n",
    "# num bag holdout 6\n",
    "# bag_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20231111_101118/\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20231111_101118/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 22.5.0: Thu Jun  8 22:22:20 PDT 2023; root:xnu-8796.121.3~7/RELEASE_ARM64_T6000\n",
      "Disk Space Avail:   789.09 GB / 994.66 GB (79.3%)\n",
      "Train Data Rows:    19875\n",
      "Train Data Columns: 160\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and label-values can't be converted to int).\n",
      "\tLabel info (max, min, mean, stddev): (999.6, 0.0, 101.90447, 183.37967)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    18851.8 MB\n",
      "\tTrain Data (Original)  Memory Usage: 13.04 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 9): [\"('elevation:m', 15)\", \"('elevation:m', 30)\", \"('elevation:m', 45)\", \"('snow_depth:cm', 15)\", \"('snow_depth:cm', 30)\", \"('snow_depth:cm', 45)\", \"('snow_melt_10min:mm', 15)\", \"('snow_melt_10min:mm', 30)\", \"('snow_melt_10min:mm', 45)\"]\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 9 | [\"('elevation:m', 15)\", \"('elevation:m', 30)\", \"('elevation:m', 45)\", \"('snow_depth:cm', 15)\", \"('snow_depth:cm', 30)\", ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 151 | [\"('absolute_humidity_2m:gm3', 0)\", \"('absolute_humidity_2m:gm3', 15)\", \"('absolute_humidity_2m:gm3', 30)\", \"('absolute_humidity_2m:gm3', 45)\", \"('air_density_2m:kgm3', 0)\", ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 150 | [\"('absolute_humidity_2m:gm3', 0)\", \"('absolute_humidity_2m:gm3', 15)\", \"('absolute_humidity_2m:gm3', 30)\", \"('absolute_humidity_2m:gm3', 45)\", \"('air_density_2m:kgm3', 0)\", ...]\n",
      "\t\t('int', ['bool']) :   1 | [\"('elevation:m', 0)\"]\n",
      "\t0.5s = Fit runtime\n",
      "\t151 features in original data used to generate 151 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 12.26 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
      "\t-31.0088\t = Validation score   (-mean_absolute_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.43s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ...\n",
      "\t-30.9136\t = Validation score   (-mean_absolute_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.43s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-21.7889\t = Validation score   (-mean_absolute_error)\n",
      "\t38.96s\t = Training   runtime\n",
      "\t34.77s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-23.0878\t = Validation score   (-mean_absolute_error)\n",
      "\t64.94s\t = Training   runtime\n",
      "\t15.7s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ...\n",
      "\t-25.2551\t = Validation score   (-mean_absolute_error)\n",
      "\t26.67s\t = Training   runtime\n",
      "\t0.88s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-23.3462\t = Validation score   (-mean_absolute_error)\n",
      "\t334.28s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ...\n",
      "\t-24.9191\t = Validation score   (-mean_absolute_error)\n",
      "\t5.58s\t = Training   runtime\n",
      "\t0.88s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-24.9663\t = Validation score   (-mean_absolute_error)\n",
      "\t11.22s\t = Training   runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-23.9128\t = Validation score   (-mean_absolute_error)\n",
      "\t193.12s\t = Training   runtime\n",
      "\t8.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-24.12\t = Validation score   (-mean_absolute_error)\n",
      "\t17.88s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.6925\t = Validation score   (-mean_absolute_error)\n",
      "\t213.59s\t = Training   runtime\n",
      "\t100.43s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-21.5056\t = Validation score   (-mean_absolute_error)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 9 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.3433\t = Validation score   (-mean_absolute_error)\n",
      "\t4.2s\t = Training   runtime\n",
      "\t0.39s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.0901\t = Validation score   (-mean_absolute_error)\n",
      "\t3.07s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ...\n",
      "\t-21.5696\t = Validation score   (-mean_absolute_error)\n",
      "\t31.9s\t = Training   runtime\n",
      "\t0.93s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.326\t = Validation score   (-mean_absolute_error)\n",
      "\t23.74s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ...\n",
      "\t-21.6244\t = Validation score   (-mean_absolute_error)\n",
      "\t6.14s\t = Training   runtime\n",
      "\t0.91s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.0245\t = Validation score   (-mean_absolute_error)\n",
      "\t11.45s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-21.9216\t = Validation score   (-mean_absolute_error)\n",
      "\t6.39s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-22.3402\t = Validation score   (-mean_absolute_error)\n",
      "\t13.98s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-21.9641\t = Validation score   (-mean_absolute_error)\n",
      "\t14.42s\t = Training   runtime\n",
      "\t0.52s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ...\n",
      "\t-21.3216\t = Validation score   (-mean_absolute_error)\n",
      "\t0.16s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1080.4s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231111_101118/\")\n"
     ]
    }
   ],
   "source": [
    "predictor3 = TabularPredictor(label=\"pv_measurement\", eval_metric='mean_absolute_error' ).fit(\n",
    "    train3,\n",
    "\n",
    "    presets='best_quality', \n",
    "    # time_limit=60*60*1.5\n",
    "    # hyperparameters='very_large', \n",
    "    # tuning_data = tuning3,\n",
    "    # use_bag_holdout=True,\n",
    "    # num_bag_folds= 6,\n",
    "    # refit_full = True,\n",
    "    # auto_stack = True,\n",
    "    # num_bag_sets= 10,\n",
    "    # set_best_to_refit_full= True,\n",
    "    # num_stack_levels = 2,\n",
    "    # verbosity = 3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = pipin.get_test_data(\"A\")\n",
    "test2 = pipin.get_test_data(\"B\")\n",
    "test3 = pipin.get_test_data(\"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data1 = TabularDataset(test1)\n",
    "test_data2 = TabularDataset(test2)\n",
    "test_data3 = TabularDataset(test3)\n",
    "\n",
    "# pred1 = pd.DataFrame(predictor1.predict(test_data1))\n",
    "pred2 = pd.DataFrame(predictor2.predict(test_data2))\n",
    "pred3 = pd.DataFrame(predictor3.predict(test_data3))\n",
    "\n",
    "negatives_pred1 = pred1[pred1[\"pv_measurement\"] < 0]\n",
    "negatives_pred2 = pred2[pred2[\"pv_measurement\"] < 0]\n",
    "negatives_pred3 = pred3[pred3[\"pv_measurement\"] < 0]\n",
    "neg = pd.concat([negatives_pred1, negatives_pred2, negatives_pred3])\n",
    "neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.concat([pred1, pred2, pred3])\n",
    "final_prediction = pipin.post_processing(pred, prediction_column=\"pv_measurement\")\n",
    "file_name = f\"gluon_3_pivot_{threshold1}_{threshold2}_{threshold3}\"\n",
    "file_name = f\"WINNER.csv\"\n",
    "final_prediction.to_csv(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gluon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
